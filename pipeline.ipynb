{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing, svm, metrics, tree, decomposition, svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Lasso, Ridge, Perceptron, SGDClassifier, OrthogonalMatchingPursuit\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import plot_confusion_matrix, accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid, KFold, StratifiedKFold\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from datetime import datetime\n",
    "from sqlalchemy import (create_engine, MetaData, Table, Column, insert, func,\n",
    "                        Integer, String, Numeric, DateTime, Enum)\n",
    "from sqlalchemy.dialects.postgresql import UUID, JSONB\n",
    "import xgboost as xgb\n",
    "import hashlib\n",
    "\n",
    "import pickle\n",
    "from statistics import mean\n",
    "import random\n",
    "import json\n",
    "import uuid\n",
    "import enum\n",
    "\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_fcns = {\n",
    "    'DC': DummyClassifier(),\n",
    "    'RF': RandomForestClassifier(n_jobs=-1, random_state=42),\n",
    "    'ET': ExtraTreesClassifier(n_jobs=-1, criterion='entropy',random_state=42),\n",
    "    'AB': AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), algorithm=\"SAMME\", random_state=42),\n",
    "    'LR': LogisticRegression(random_state=42,solver='liblinear'),\n",
    "    'SVM': svm.SVC(kernel='linear', probability=True, random_state=42),\n",
    "    'GB': GradientBoostingClassifier(random_state=42),\n",
    "    'NB': GaussianNB(),\n",
    "    'DT': DecisionTreeClassifier(random_state=42),\n",
    "    'SGD': SGDClassifier(loss=\"hinge\", random_state=42),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'LRR': Ridge(random_state=42),\n",
    "    'LRL': Lasso(random_state=42),\n",
    "    'XGB': xgb.XGBClassifier(n_jobs=-1,random_state=42)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        experiment_group_id,\n",
    "        experiment_config_name,\n",
    "        dataset_name,\n",
    "        X,\n",
    "        y,\n",
    "        clfs,\n",
    "        hyperparameters,\n",
    "        split_methods,\n",
    "        split_random_states,\n",
    "        metrics,\n",
    "        db=False,\n",
    "        sub_groups=None,\n",
    "    ):\n",
    "        self.experiment_group_id = experiment_group_id\n",
    "        self.experiment_config_name = experiment_config_name\n",
    "        self.dataset_name = dataset_name\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.split_random_states = split_random_states\n",
    "        self.split_methods = split_methods\n",
    "        self.set_clfs(clfs)\n",
    "        self.set_hyperparemeters(hyperparameters)\n",
    "        self.db = db\n",
    "        self.metrics = metrics\n",
    "        self.sub_groups = sub_groups\n",
    "        self.results_df = pd.DataFrame(\n",
    "            columns=(\n",
    "                \"experiment_output_id\",\n",
    "                \"experiment_group_id\",\n",
    "                \"experiment_config_name\",\n",
    "                \"dataset_name\",\n",
    "                \"model_type\",\n",
    "                \"model_parameters\",\n",
    "                \"sub_group\",\n",
    "                \"metric_name\",\n",
    "                \"metric_k\",\n",
    "                \"metric_score\",\n",
    "                \"split_method\",\n",
    "                \"split_seed\",\n",
    "                \"split_percentage\",\n",
    "                \"split_num_fold\",\n",
    "                \"split_num\",\n",
    "                \"created_at\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Check for presets\n",
    "    def set_hyperparemeters(self, hyperparemeters):\n",
    "        if hyperparemeters == \"small\":\n",
    "            self.hyperparemeters = small_grid\n",
    "        elif hyperparemeters == \"large\":\n",
    "            self.hyperparemeters = large_grid\n",
    "        elif hyperparemeters == \"test\":\n",
    "            self.hyperparemeters = test_grid\n",
    "        else:\n",
    "            self.hyperparemeters = hyperparemeters\n",
    "\n",
    "    # Check for presets\n",
    "    def set_clfs(self, clfs):\n",
    "        if clfs == \"all\":\n",
    "            self.clfs = clf_list_all\n",
    "        elif clfs == \"test\":\n",
    "            self.clfs = clf_list_test\n",
    "        else:\n",
    "            self.clfs = clfs\n",
    "\n",
    "    def save_ouput(\n",
    "        self,\n",
    "        scores_obj,\n",
    "        model_type,\n",
    "        model_parameters,\n",
    "        sub_group_metadata,\n",
    "        split_method,\n",
    "        split_seed,\n",
    "        split_percentage,\n",
    "        split_num_fold,\n",
    "        split_num,\n",
    "    ):\n",
    "\n",
    "        experiment_group_id = self.experiment_group_id\n",
    "        experiment_config_name = self.experiment_config_name\n",
    "        dataset_name = self.dataset_name\n",
    "\n",
    "        if sub_group_metadata:\n",
    "            sub_group_json = json.dumps(dict(sub_group_metadata))\n",
    "        else:\n",
    "            sub_group_json = None\n",
    "\n",
    "        model_parameters_json = json.dumps(model_parameters)\n",
    "\n",
    "        for metric_key in scores_obj:\n",
    "            if \"@\" in metric_key:\n",
    "                metric_name, metric_k = metric_key.split(\"@\")\n",
    "            else:\n",
    "                metric_name = metric_key\n",
    "                metric_k = None\n",
    "\n",
    "            if self.db == False:\n",
    "                now = datetime.now()\n",
    "                dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "\n",
    "                self.results_df.loc[len(self.results_df)] = [\n",
    "                    len(self.results_df),  # experiment_output_id\n",
    "                    experiment_group_id,  # experiment_group_id\n",
    "                    experiment_config_name,\n",
    "                    dataset_name,\n",
    "                    model_type,\n",
    "                    model_parameters_json,  # model_parameters\n",
    "                    sub_group_json,  # sub_group\n",
    "                    metric_name,\n",
    "                    metric_k,  # metric_score\n",
    "                    scores_obj[metric_key],\n",
    "                    split_method,\n",
    "                    split_seed,\n",
    "                    split_percentage,\n",
    "                    split_num_fold,\n",
    "                    split_num,\n",
    "                    dt_string,  # created_at\n",
    "                ]\n",
    "            elif self.db == True:\n",
    "                ins = experiment_outputs_table.insert().values(\n",
    "                    experiment_group_id=experiment_group_id,\n",
    "                    experiment_config_name=experiment_config_name,\n",
    "                    dataset_name=dataset_name,\n",
    "                    model_type=model_type,\n",
    "                    model_parameters=model_parameters_json,\n",
    "                    sub_group=sub_group_json,\n",
    "                    metric_name=metric_name,\n",
    "                    metric_k=metric_k,\n",
    "                    metric_score=scores_obj[metric_key],\n",
    "                    split_method=split_method,\n",
    "                    split_seed=split_seed,\n",
    "                    split_percentage=split_percentage,\n",
    "                    split_num_fold=split_num_fold,\n",
    "                    split_num=split_num,\n",
    "                )\n",
    "                result = engine.execute(ins)\n",
    "\n",
    "    def train_models(self):\n",
    "        # Initialize the evaluator\n",
    "        evaluator = ModelEvaluator(metrics=self.metrics)\n",
    "\n",
    "        print(\"DEBUG: Starting model training\")\n",
    "\n",
    "        models_to_run = list(self.clfs.keys())\n",
    "        grid = self.hyperparemeters\n",
    "        i = 0\n",
    "        for index, clf in enumerate([self.clfs[x] for x in models_to_run]):\n",
    "            parameter_values = grid[models_to_run[index]]\n",
    "            for p in ParameterGrid(parameter_values):\n",
    "                clf.set_params(**p)\n",
    "                i += 1\n",
    "                print(f\"DEBUG: {self.dataset_name}: Training ( # {i} ) {clf} | {p}\")\n",
    "\n",
    "                for split_seed in self.split_random_states:\n",
    "                    for split_method in self.split_methods:\n",
    "\n",
    "                        if split_method.find(\"@\") < 0:\n",
    "                            raise ValueError(\"You must define a number of folds or percentage for the split functions.\")\n",
    "                        else:\n",
    "                            split_function = split_method[0 : split_method.find(\"@\")]\n",
    "                            split_num_fold = int(split_method[split_method.find(\"@\") + 1 :])\n",
    "\n",
    "                        if split_function == \"train_test_split\":\n",
    "                            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                self.X, self.y, test_size=split_num_fold / 100, random_state=split_seed\n",
    "                            )\n",
    "                            clf.fit(X_train, y_train)\n",
    "\n",
    "                            evaluator.load(X_test=X_test, y_true=y_test, clf=clf)\n",
    "                            scores_obj = evaluator.get_metrics()\n",
    "                            self.save_ouput(\n",
    "                                scores_obj=scores_obj,\n",
    "                                model_type=models_to_run[index],\n",
    "                                model_parameters=p,\n",
    "                                sub_group_metadata=None,\n",
    "                                split_method=split_function,\n",
    "                                split_seed=split_seed,\n",
    "                                split_percentage=split_num_fold,\n",
    "                                split_num_fold=None,\n",
    "                                split_num=1,\n",
    "                            )\n",
    "\n",
    "                            # Subgroup metrics\n",
    "                            for sub_group_var in self.sub_groups:\n",
    "                                for sub_group_value in list(X_test[sub_group_var].unique()):\n",
    "                                    X_test_temp = X_test[X_test[sub_group_var] == sub_group_value]\n",
    "                                    y_test_temp = y_test.loc[X_test[sub_group_var] == sub_group_value]\n",
    "\n",
    "                                    evaluator.load(X_test=X_test_temp, y_true=y_test_temp, clf=clf)\n",
    "                                    scores_obj = evaluator.get_metrics()\n",
    "                                    sub_group_dict = {sub_group_var: int(sub_group_value)}\n",
    "\n",
    "                                    self.save_ouput(\n",
    "                                        scores_obj=scores_obj,\n",
    "                                        model_type=models_to_run[index],\n",
    "                                        model_parameters=p,\n",
    "                                        sub_group_metadata=sub_group_dict,\n",
    "                                        split_method=split_function,\n",
    "                                        split_seed=split_seed,\n",
    "                                        split_percentage=split_num_fold,\n",
    "                                        split_num_fold=None,\n",
    "                                        split_num=1,\n",
    "                                    )\n",
    "\n",
    "                        elif split_function == \"StratifiedKFold\" or split_function == \"KFold\":\n",
    "\n",
    "                            kf = eval(\n",
    "                                f\"{split_function}(n_splits={split_num_fold}, random_state={split_seed}, shuffle=True)\"\n",
    "                            )\n",
    "                            split_num = 0\n",
    "                            for train_index, test_index in kf.split(self.X, self.y):\n",
    "                                split_num += 1\n",
    "                                X_train, X_test = self.X.iloc[train_index], self.X.iloc[test_index]\n",
    "                                y_train, y_test = self.y[train_index], self.y[test_index]\n",
    "                                clf.fit(X_train, y_train)\n",
    "\n",
    "                                # Here we run the model evalutor and save the stats\n",
    "                                evaluator.load(X_test=X_test, y_true=y_test, clf=clf)\n",
    "                                scores_obj = evaluator.get_metrics()\n",
    "                                self.save_ouput(\n",
    "                                    scores_obj=scores_obj,\n",
    "                                    model_type=models_to_run[index],\n",
    "                                    model_parameters=p,\n",
    "                                    sub_group_metadata=None,\n",
    "                                    split_method=split_function,\n",
    "                                    split_seed=split_seed,\n",
    "                                    split_percentage=None,\n",
    "                                    split_num_fold=split_num_fold,\n",
    "                                    split_num=split_num,\n",
    "                                )\n",
    "\n",
    "                                # Subgroup metrics\n",
    "                                for sub_group_var in self.sub_groups:\n",
    "                                    for sub_group_value in list(X_test[sub_group_var].unique()):\n",
    "                                        X_test_temp = X_test[X_test[sub_group_var] == sub_group_value]\n",
    "                                        y_test_temp = y_test.loc[X_test[sub_group_var] == sub_group_value]\n",
    "\n",
    "                                        evaluator.load(X_test=X_test_temp, y_true=y_test_temp, clf=clf)\n",
    "                                        scores_obj = evaluator.get_metrics()\n",
    "\n",
    "                                        sub_group_dict = {sub_group_var: int(sub_group_value)}\n",
    "\n",
    "                                        self.save_ouput(\n",
    "                                            scores_obj=scores_obj,\n",
    "                                            model_type=models_to_run[index],\n",
    "                                            model_parameters=p,\n",
    "                                            sub_group_metadata=sub_group_dict,\n",
    "                                            split_method=split_function,\n",
    "                                            split_seed=split_seed,\n",
    "                                            split_percentage=None,\n",
    "                                            split_num_fold=split_num_fold,\n",
    "                                            split_num=split_num,\n",
    "                                        )\n",
    "        print(\"DEBUG: Model training complete\")\n",
    "\n",
    "\n",
    "class ModelEvaluator:\n",
    "    def __init__(self, clf=None, X_test=None, y_true=None, metrics=None):\n",
    "        self.clf = clf\n",
    "        self.X_test = X_test\n",
    "        self.y_true = y_true\n",
    "        self.metrics = metrics\n",
    "\n",
    "    def load(self, X_test, y_true, clf):\n",
    "        self.X_test = X_test\n",
    "        self.y_true = y_true\n",
    "        self.clf = clf\n",
    "        if (type(self.clf) == type(Ridge())) or (type(self.clf) == type(Lasso())):\n",
    "            self.y_score = self.clf.predict(self.X_test)\n",
    "            self.y_pred = np.where(self.y_score >= 0.5, 1, 0)\n",
    "        else:\n",
    "            self.y_pred = self.clf.predict(self.X_test)\n",
    "            self.y_score = self.clf.predict_proba(self.X_test)[:, 1]\n",
    "\n",
    "    # Check for presets\n",
    "    def set_metrics(self, metrics):\n",
    "        if metrics == \"small\":\n",
    "            self.metrics = metric_list_small\n",
    "        elif metrics == \"test\":\n",
    "            self.metrics = metric_list_test\n",
    "        else:\n",
    "            self.metrics = metrics\n",
    "\n",
    "    def metric_at_k(self, metric, k):\n",
    "        y_pred = np.where(self.y_score > np.percentile(self.y_score, (100 - k)), 1, 0)\n",
    "        s = eval(metric + \"(self.y_true,y_pred)\")\n",
    "        return s\n",
    "\n",
    "    def get_metrics(self):\n",
    "        results = {}\n",
    "        for metric in self.metrics:\n",
    "            if metric.find(\"@\") > -1:\n",
    "                m = metric[0 : metric.find(\"@\")]\n",
    "                k = int(metric[metric.find(\"@\") + 1 :])\n",
    "                s = self.metric_at_k(metric=m, k=k)\n",
    "            else:\n",
    "                s = eval(metric + \"(self.y_true,self.y_pred)\")\n",
    "            results[metric] = s\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read json\n",
    "root_list = !echo \"${HOME}/ml-explainability\"\n",
    "root_path = str(root_list[0])\n",
    "\n",
    "config_name = 'baseline_experiment_drugs_all_models'\n",
    "config_path = f'{root_path}/configs/{config_name}.json'\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "clfs = {}\n",
    "for clf in config['hyperparameters']:\n",
    "    clfs[clf] = clf_fcns[clf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Postgres node address\n",
    "postgres_node_file = '/scratch/isk273/postgres_node.txt'\n",
    "with open(postgres_node_file, 'r') as f:\n",
    "    postgres_address = f.read().replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSTGRES_ADDRESS = postgres_address\n",
    "POSTGRES_PORT = '5432'\n",
    "POSTGRES_USERNAME = 'isk273'\n",
    "POSTGRES_PASSWORD = 'andrewIan'\n",
    "POSTGRES_DBNAME = 'explainability_db'\n",
    "\n",
    "postgres_str = (\n",
    "    f'postgresql://{POSTGRES_USERNAME}:{POSTGRES_PASSWORD}@{POSTGRES_ADDRESS}:{POSTGRES_PORT}/{POSTGRES_DBNAME}'\n",
    ")\n",
    "\n",
    "# Create the connection\n",
    "engine = create_engine(postgres_str, echo = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetNameEnum(enum.Enum):\n",
    "    education = 1\n",
    "    healthcare = 2\n",
    "    housing = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitMethodEnum(enum.Enum):\n",
    "    StratifiedKFold = 1\n",
    "    KFold = 2\n",
    "    train_test_split = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = MetaData(engine)\n",
    "experiment_outputs_table = Table(\n",
    "    \"experiment_outputs\",\n",
    "    metadata,\n",
    "    Column(\"experiment_output_id\", UUID(as_uuid=True), primary_key=True, default=uuid.uuid4),\n",
    "    Column(\"experiment_group_id\", String, nullable=False),\n",
    "    Column(\"experiment_config_name\", String, nullable=False),\n",
    "    Column(\"dataset_name\", Enum(DatasetNameEnum), nullable=False),\n",
    "    Column(\"model_type\", String(3), nullable=False),\n",
    "    Column(\"model_parameters\", JSONB, nullable=False),\n",
    "    Column(\"sub_group\", JSONB),\n",
    "    Column(\"metric_name\", String, nullable=False),\n",
    "    Column(\"metric_k\", Integer),\n",
    "    Column(\"metric_score\", Numeric, nullable=False),\n",
    "    Column(\"split_method\", Enum(SplitMethodEnum), nullable=False),\n",
    "    Column(\"split_seed\", Integer, nullable=False),\n",
    "    Column(\"split_num_fold\", Integer),\n",
    "    Column(\"split_percentage\", Integer),\n",
    "    Column(\"split_num\", Integer, nullable=False),\n",
    "    Column(\"created_at\", DateTime(timezone=True), server_default=func.now(), nullable=False),\n",
    ")\n",
    "\n",
    "metadata.create_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_group_id = hashlib.md5((str(config)+datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")).encode('utf-8')).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "for i in range(0, len(config[\"datasets\"])):\n",
    "    dataset_name = config[\"datasets\"][i]\n",
    "    X_path = f\"{root_path}/{config['X'][dataset_name]}\"\n",
    "    y_path = f\"{root_path}/{config['y'][dataset_name]}\"\n",
    "\n",
    "    X = pd.read_csv(X_path, index_col=0)\n",
    "    print(f\"DEBUG: {dataset_name}: {X.shape[0]} rows | {X.shape[1]} cols\")\n",
    "    y = pd.read_csv(y_path, index_col=0, squeeze=True)\n",
    "    try:\n",
    "        sub_groups = config[\"sub_groups\"][dataset_name]\n",
    "    except KeyError:\n",
    "        sub_groups = None\n",
    "    trainer = Trainer(\n",
    "        experiment_config_name=config_name,\n",
    "        experiment_group_id=experiment_group_id,\n",
    "        dataset_name=dataset_name,\n",
    "        X=X,\n",
    "        y=y,\n",
    "        clfs=clfs,\n",
    "        hyperparameters=config[\"hyperparameters\"],\n",
    "        split_methods=config[\"split_methods\"],\n",
    "        split_random_states=config[\"split_random_states\"],\n",
    "        metrics=config[\"metrics\"],\n",
    "        sub_groups=sub_groups,\n",
    "        db=config[\"save_to_db\"],\n",
    "    )\n",
    "    trainer.train_models()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "61714080e0d57c115cde96a01d9d15a079d2d743fe17300954299296174f26d5"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
